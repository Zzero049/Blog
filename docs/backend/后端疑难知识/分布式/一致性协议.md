# 一致性理论

## CAP理论

一个分布式系统不可能同时满足 **Consistency**（一致性），**Availability**（可用性），**Partition tolerance**（分区容错性）这三个基本需求，最多只能同时满足其中**两项**。

**一致性**

在分布式环境，一致性是指数据在**多个副本之间保持一致的特性**。在数据一致的某个时间点，执行更新操作后，也要求系统各部分数据是一致的。

**可用性**

可用性是指，系统对于外部的请求，必须在一定可容忍**有限时间内给出应答。**

**分区容错性**

首先要了解网络分区，即在分布式系统内，不同节点归属在不同网络子网，子网络之间可能出现网络不连通情况，但在子网内部的通信是正常的。

分区容错性，约束了分布式系统在遭遇任何网络分区后，仍要对外提供一致性和可用性的服务，除非所有环境都出现故障。

**怎么理解Partition tolerance？**

1. 一个网络正常的分布式系统中，节点之间是互相连通的。当发生网络故障时，部分节点之间不连通，整个网络被分为了几块区域。称之为**分区**。

2. 假设当前网络被分为A、B两个分区。当需要访问的数据**只存储在A分区的某一个节点**上时，此时B分区的节点无法访问这些数据，此时整个系统不具有分区容忍性。比如有一个服务，3个节点在中国，2个节点在美国，假设中美海底电缆全部断掉（发生分区），中国这三个节点依然可以提供服务，美国两个节点也可以提供服务，为了保证一致性的话，就让其中一个分区失去服务，这时不具有分区容错性；如果让两个分区各自服务，那么具备分区容错

3. 要提高分区容忍性的办法是**将数据复制在多个节点上**，这样在网络分区后，各个分区中包含存储了该数据的节点的可能性就提高了，分区容忍性也提高了。但是带来的问题是，多个节点上的数据难以保持一致性，若要保持一致性，每次写操作需要等待全部节点都写成功，影响了系统的可用性。

   

**实际使用中如何做出取舍？**

1. 在分布式系统中，网络问题是一个必定会出现的异常情况。因此，partition tolerance几乎是一个分布式系统必须要面对和解决的问题。
2. 在设计分布式系统时，开发人员往往需要把精力花在如何根据业务特点在**Consistency和Availability中做出取舍**。

![img](https://gitee.com/zero049/MyNoteImages/raw/master/692462-20190407090815236-177165564.png)

## BASE理论

从客户端角度，多进程并发访问时，更新过的数据在不同进程如何获取的不同策略，决定了不同的一致性。对于关系型数据库，要求更新过的数据能被后续的访问都能看到，这是强一致性。如果能容忍后续的部分或者全部访问不到，则是弱一致性。如果经过一段时间后要求能访问到更新后的数据，则是最终一致性。
BASE是Basically Available（基本可用）、Soft state（软状态）和Eventually consistent（最终一致性）三个短语的缩写。权衡了CAP理论中的A和C。

**基本可用**

当出现故障时，允许出现部分不可用，这不等同于系统整体不可用。基本可用，可以体现在**响应时间上的损失**和**功能上的损失**。在大促期间，流量激增，活动页的响应时间会数倍增长；很多耗时服务，可能由于压力被降级到一些轻量的降级预案。这些措施只会影响轻微的体验和系统目标，而不会带来整体不可用的灾难后果。

**软状态**

软状态是指，系统中的节点数据都存在中间状态，该中间状态的存在不会影响到整体的可用性，这通常体现在数据在一致性同步复制过程中，允许存在延时。

**最终一致性**

**数据在同步复制中虽然存在延时，但终究会达到最终的一致。**最终一致性是一种弱一致性，数据一定能够达到最终一致的状态，所有客户端最终也可以得到最新的值。

总的来说，BASE理论面向的是大型高可用可扩展的分布式系统，和传统的事物ACID特性是相反的，它完全不同于ACID的强一致性模型，而是通过牺牲强一致性来获得可用性，并允许数据在一段时间内是不一致的，但最终达到一致状态。但同时，在实际的分布式场景中，不同业务单元和组件对数据一致性的要求是不同的，因此在具体的分布式系统架构设计过程中，ACID特性和BASE理论往往又会结合在一起。



## 解决分布式事务一致性问题的方案

### 2PC

2PC即Two-Phase Commit，二阶段提交。广泛应用在数据库领域，为了使得基于分布式架构的所有节点可以在进行事务处理时能够保持原子性和一致性。绝大部分关系型数据库，都是基于2PC完成分布式的事务处理。

首先2PC协议中有两类角色：**一个中心化协调者节点**（coordinator）和**多个参与者节点**（partcipant）。协调者负责提交事务请求，并发出事务提交还是回滚操作给参与者执行。而参与者对协调者发送的事务请求作出响应，反馈Yes或No，最后执行协调者发出的commit或rollback操作。2PC是将事务的提交过程分成了**两个阶段**来进行处理，分为**提交事务请求、执行事务提交**

#### 阶段一：提交事务请求

1. 事务询问。协调者向所有参与者发送事务内容，询问是否可以执行提交操作，并开始等待各参与者进行响应；
2. 执行事务。各参与者节点，执行事务操作，并将Undo和Redo操作计入本机事务日志；
3. 各参与者向协调者反馈事务问询的响应。成功执行返回Yes，否则返回No。

![img](https://gitee.com/zero049/MyNoteImages/raw/master/1090617-20190710222443794-591603727.jpg)

#### 阶段二：执行事务提交

协调者在阶段二决定是否最终执行事务提交操作。这一阶段包含两种情形：

**1、执行事务提交**

所有参与者reply Yes，那么执行事务提交。

1. 发送提交请求。协调者向所有参与者发送Commit请求；
2. 事务提交。参与者收到Commit请求后，会**正式执行事务提交操作**，并在完成提交操作之后，释放在整个事务执行期间占用的资源；
3. 反馈事务提交结果。参与者在完成事务提交后，写协调者发送Ack消息确认；
4. 完成事务。协调者在收到所有参与者的Ack后，完成事务。

**2、中断事务，进行回滚**

事情总会出现意外，当存在某一参与者向协调者发送No响应，或者等待超时。协调者只要无法收到所有参与者的Yes响应，就会中断事务。

1. 发送回滚请求。协调者向所有参与者发送Rollback请求；
2. 回滚。参与者收到请求后，利用本机Undo信息，执行Rollback操作。并在回滚结束后释放该事务所占用的系统资源；
3. 反馈回滚结果。参与者在完成回滚操作后，向协调者发送Ack消息；
4. 中断事务。协调者收到所有参与者的回滚Ack消息后，完成事务中断。

![img](https://gitee.com/zero049/MyNoteImages/raw/master/1090617-20190710222454275-1462865655.jpg)

#### 优缺点

**优点：**

- 主要体现在实现原理简单；

**缺点：**

- 在阶段二时，参与者一直是等待协调者发出commit或rollback请求，参与该事务操作的逻辑都处于**阻塞状态，占用事务资源**，也就是说，各个参与者都在等待其他参与者响应，无法进行其他操作；
- 协调者是个单点，一旦出现问题，其他参与者将无法释放事务资源，也无法完成事务操作；（**单点故障**）
- **数据可能不一致**。当执行事务提交过程中，如果协调者向所有参与者发送Commit请求后，发生局部网络异常或者协调者在尚未发送完Commit请求，即出现崩溃，最终导致只有部分参与者收到、执行请求。于是整个系统将会出现数据不一致的情形；（比如协调者与参与者ABC，AB收到commit，C由于网络故障或者协调者挂了，没收到commit，就没提交，出现数据不一致）
- 保守。2PC**没有完善的容错机制**，当参与者出现故障时，协调者无法快速得知这一失败，只能**严格依赖超时设置**来决定是否进一步的执行提交还是中断事务。

### 3PC

针对2PC的缺点，研究者提出了3PC，即Three-Phase Commit。作为2PC的改进版，3PC将原有的两阶段过程，重新划分为CanCommit、PreCommit和do Commit三个阶段。**CanCommit、PreCommit类似把2PC的提交事务请求划分成两个步骤**

![三阶段提交示意图](https://gitee.com/zero049/MyNoteImages/raw/master/692462-20190407142856081-1418301502.png)

#### **阶段一：CanCommit**

1. 事务询问。协调者向所有参与者发送包含事务内容的canCommit的请求，询问是否可以执行事务提交，并等待应答；
2. 各参与者反馈事务询问。正常情况下，如果参与者认为可以顺利执行事务，则返回Yes，否则返回No。

#### **阶段二：PreCommit**

在本阶段，协调者会根据上一阶段的反馈情况来决定是否可以执行事务的PreCommit操作。有以下两种可能：

**1、<font color="red">执行事务预提交</font>（其实就是2PC的执行事务请求阶段）**

1. 发送预提交请求。协调者向所有节点发出PreCommit请求，并进入prepared阶段；
2. 事务预提交。参与者收到PreCommit请求后，会执行事务操作，并将Undo和Redo日志写入本机事务日志；
3. 各参与者成功执行事务操作，同时将反馈以Ack响应形式发送给协调者，同事等待最终的Commit或Abort指令。

**2、中断事务**
加入任意一个参与者向协调者发送No响应，或者等待超时，协调者在没有得到所有参与者响应时，即可以中断事务：

1. 发送中断请求。 协调者向所有参与者发送Abort请求；
2. 中断事务。无论是收到协调者的Abort请求，还是等待协调者请求过程中**出现超时**，参与者都会中断事务；

#### **阶段三：doCommit**

在这个阶段，会真正的进行事务提交，同样存在两种可能。

**1、执行提交（参与者收到反馈超时自动提交）**

一旦进入阶段三，如果协调者出现问题或者协调者与参与者之间的网络出现故障，那么**参与者都会在等待超时之后，继续执行事务的提交**

1. 发送提交请求。假如协调者收到了所有参与者的Ack响应，那么将从预提交转换到提交状态，并向所有参与者，发送doCommit请求；
2. 事务提交。参与者收到doCommit请求后，会正式执行事务提交操作，并在完成提交操作后释放占用资源；
3. 反馈事务提交结果。参与者将在完成事务提交后，向协调者发送Ack消息；
4. 完成事务。协调者接收到所有参与者的Ack消息后，完成事务。

**2、中断事务，事务回滚**

在该阶段，假设**上一阶段**正常状态的协调者接收到任一个参与者发送的**No响应**，或在**超时**时间内，仍旧没收到反馈消息，**协调者就会中断事务：**

1. 发送中断请求。协调者向所有的参与者发送abort请求；
2. 事务回滚。参与者收到abort请求后，会利用阶段二中的Undo消息执行事务回滚，并在完成回滚后释放占用资源；
3. 反馈事务回滚结果。参与者在完成回滚后向协调者发送Ack消息；
4. 中端事务。协调者接收到所有参与者反馈的Ack消息后，完成事务中断。

#### 优缺点

**优点**

**降低了参与者的阻塞范围**，参与者不会一直被阻塞，假如了超时机制，如果参与者一直没有收到协调者的PreCommit请求，则会自己中断事务，并在阶段三中，如果协调者宕机，参与者依然会进行事务的提交，**解决了2PC中的单点问题**。

**缺点**

在阶段三协调者出现问题（**比如说一个参与者发送中止响应，但是协调者宕机，其他事务依然提交**），参与者依然执行事务的提交，这必然会出现数据的不一致性



## 弱一致性

数据更新后，如果能容忍后续的访问只能访问到部分或者全部访问不到，也不保证以后一定能访问到，则是弱一致性。

### 最终一致性

不保证在任意时刻任意节点上的同一份数据都是相同的，但是随着时间的迁移，不同节点上的同一份数据总是在向趋同的方向变化。

- Gossip（Cassandra的通信协议）

- DNS（Domain Name System）

  比如我现在要创一个域名，并绑定一个ip（用阿里云或者什么的服务），这时候某个本地域名服务器这个新的记录更新给其他域名服务器，但是需要一定的时间，这时候如果有其他用户想访问这个网站，可以DNS解析失败，因为请求的DNS服务器还没更新到

  ![image-20200925222547465](https://gitee.com/zero049/MyNoteImages/raw/master/image-20200925222547465.png)





## 强一致性算法

**在任意时刻，所有节点中的数据是一样的。**其实只有两类数据一致性，强一致性与弱一致性。强一致性也叫做**线性一致性**，除此以外，所有其他的一致性都是**弱一致性的特殊情况。所谓强一致性，即复制是同步的，弱一致性，即复制是异步的。**保证系统改变提交以后立即改变集群的状态。

### 主从同步复制

该算法流程如下

1.Master接受写请求

2.Master复制日志至 slave

3.**Master等待，直到从库返回**

![image-20200925223521196](https://gitee.com/zero049/MyNoteImages/raw/master/image-20200925223521196.png)

**问题：**

一个节点失败，Master阻塞（Master必须等待从节点接受日志修改并返回），导致整个集群不可用，保证了一致性，可用性却大大降低

### 多数派思想

每次写都保证写入大于`N/2`个节点，每次读保证从大于`N/2`个节点中读。

但是在并发环境下，无法保证系统正确性，顺序非常重要

如下图，有三个节点，其中第三个节点两条操作和前两个不同，这样数据就不一致了

![image-20200925224204801](https://gitee.com/zero049/MyNoteImages/raw/master/image-20200925224204801.png)



### Basic Paxos算法

为描述 Paxos算法，Lamport虚拟了一个叫做 Paxos的希腊城邦，这个岛按照议会民主制的政治模式制订法律，但是没有人愿意将自己的全部时间和精力放在这种事。所以无论是议员，议长或者传递纸条的服务员都不能承诺别人需要时一定会出现，也无法承诺批准决议或者传递消息的时间。

#### 角色介绍

**Client：**系统外部角色，请求发起者。**像民众。**

**Propser：**接受 Client请求，向集群提出提议（propose）。并在冲突发生时，起到冲突调节的作用。**像议员**，替民众提出议案。

**Accpetor（Voter）：**提议投票和接收者，只有在形成法定人数（Quorum，一般即为majority多数派）时，提议才会最终被接受。**像国会。**

**Learner：**提议接受者，backup，备份，对集群一致性没什么影响。**像记录员。**

#### 步骤

**阶段一：**

**1、Prepare（产生提案）**

Propser准备一个N号提案，若此N大于这个 proposer之前提出提案编号，请求 acceptors的 quorum接受。（说明是新提案）

**2、Promise（确认提案没过时，没过时就接受）**

如果N大于此 acceptor之前接受的任何提案编号则接受，否则拒绝。（N如果在比acceptor接受的最高编号都小，那么说明提案过时了）

**阶段二：**

**3、Accept（如果acceptor接受的达到多数派，提案上传）**

Acceptor开始表决，如果达到了多数派，proposer会发出 accept请求，此请求包含提案编号N，以及提案内容。

**4、Accepted（如果没有新的提案通过，那就通过这个提案并记录）**

如果此 acceptor在此期间没有收到任何编号大于N的提案，则接受此提案内容，Learner记录提案，否则忽略。

可以如下图所示：

![image-20200925232955771](https://gitee.com/zero049/MyNoteImages/raw/master/image-20200925232955771.png)

对应到现实理解的话：

可以理解为Proposer相当于我们服务器，现在想写入一条数据，Acceptor集里面都是数据库，且相互间共识，而Learner相当于只是一个备份的数据库，不参与投票。现在想写入一条数据，只把一个编号发给共识数据库集中的数据库，当他们多数同意了（解决单点阻塞问题），我就把这条具体的数据写入到所有的数据库中，包括learner



#### 异常情况

**1、部分节点失败，但达到了多数派（Quoroms）**

照样可以成功

![image-20200925233114256](pictures/image-20200925233114256.png)



**2、Proposer失败**

若Proposer故障，没关系，再从集群中选出Proposer即可，不过要产生新的提案编号

![image-20200925233315196](https://gitee.com/zero049/MyNoteImages/raw/master/image-20200925233315196.png)



**3、潜在问题：活锁**

比如有两个议员，议员A现在提出提案1，投票者现在在讨论提案1，这时议员B过来，又提了个提案2，那么提案1失效，提案2进行讨论，然后议员A很不爽，又提出提案3，提案2失效，提案3进入讨论，如此往复，最终任何提案都没有通过

解决方案：

设置一个random时间，提出提案后等待这么久时间才能再提出（贤者时间）

![image-20200925233423475](pictures/image-20200925233423475.png)

Basic paxos的问题：

难实现、效率低（2轮RPC）、活锁



### Multi Paxos

提出新概念，Leader；

Leader：唯一的 propser，所有请求都需经过此 Leader（就一个propser那就没有活锁问题了）

可以形象的理解为**propser从议员升级成总统（但是还是要先从议员升级成总统）**

#### 基本流程

现在流程类似：

先选第N号总统，选出第N号总统之后，之后总统提出的提案都必须通过，而且总统有一定的任期，在任期内都不需要再选一个总统出来（减少RPC次数）

![image-20200925235333607](https://gitee.com/zero049/MyNoteImages/raw/master/image-20200925235333607.png)

角色还是太多了，我们可以减少角色，进一步简化

可以将Proposer、Acceptor和Learner三者身份**集中在一个节点上**，此时只需要从集群中选出Proposer，其他节点都是Acceptor和Learner，与其他结点类似主从关系，别的节点都听这个Proposer的

![image-20200925235702537](https://gitee.com/zero049/MyNoteImages/raw/master/image-20200925235702537.png)

Paxos算法不易理解，在实现上很困难，于是简化版的Paxos出现了——Raft

### Raft

划分成三个子问题

- Leader Election（选择leader）
- Log Replication（日志复制）
- Safety（保证共识性）

重定义角色：

- Leader，总统节点，负责提出提案
- Follower，追随者节点，负责同意Leader发出的提案
- Candidate，候选者节点，当集群中没有Leader时，从中选择一个成为Leader，可以认为是一个临时的状态

#### 步骤

Raft算法将一致性问题分解为两个的子问题，**Leader选举**和**状态复制**，动画演示链接http://thesecretlivesofdata.com/raft/

**1、Leader选举**

每个Follower都持有一个**定时器**

![image-20200926013104257](https://gitee.com/zero049/MyNoteImages/raw/master/image-20200926013104257.png)

当定时器时间到了而集群中仍然没有Leader（检查心跳，就是有leader发送消息过来就会重置定时器），Follower将声明自己是Candidate并参与Leader选举（自己投自己），同时**将消息发给其他节点来争取他们的投票**，若其他节点长时间没有响应Candidate将重新发送选举信息

![image-20200926013316822](https://gitee.com/zero049/MyNoteImages/raw/master/image-20200926013316822.png)

集群中其他结点给Candidate投票，如下图中的C

获得多数派支持的Candidate将成为**第M任Leader**（M任是最新的任期）

![image-20200926013341155](https://gitee.com/zero049/MyNoteImages/raw/master/image-20200926013341155.png)

在任期内的Leader会**不断发送心跳（也可以携带数据）**给其他节点证明自己还活着，其他节点受到心跳以后就清空自己的计时器并回复Leader的心跳。这个机制保证其他节点不会在Leader任期内参加Leader选举。

![image-20200926013514727](https://gitee.com/zero049/MyNoteImages/raw/master/image-20200926013514727.png)

![image-20200926013802567](https://gitee.com/zero049/MyNoteImages/raw/master/image-20200926013802567.png)

当Leader节点出现故障而导致Leader失联，没有接收到心跳的Follower节点将准备成为Candidate进入下一轮Leader选举

![image-20200926013900632](https://gitee.com/zero049/MyNoteImages/raw/master/image-20200926013900632.png)

![image-20200926014058486](https://gitee.com/zero049/MyNoteImages/raw/master/image-20200926014058486.png)

**冲突**：若出现两个Candidate同时选举并获得了相同的票数，那么这两个Candidate将**随机推迟一段时间**后再向其他节点发出投票请求，这保证了再次发送投票请求以后不冲突

![image-20200926014313248](https://gitee.com/zero049/MyNoteImages/raw/master/image-20200926014313248.png)

**2、状态复制**

1. Leader负责接收来自Client的提案请求**（红色提案表示未确认）**

![image-20200926015022887](https://gitee.com/zero049/MyNoteImages/raw/master/image-20200926015022887.png)

2. 提案内容将包含在Leader发出的**下一个心跳中**，Follower接收到心跳包里并拿到里面的数据写入log，回复确认给Leader

![image-20200926015046513](https://gitee.com/zero049/MyNoteImages/raw/master/image-20200926015046513.png)

3. Leader接收到多数派Follower的回复以后**确认提案**并写入自己的存储空间中并**回复Client**

   ![image-20200926015115947](https://gitee.com/zero049/MyNoteImages/raw/master/image-20200926015115947.png)

4.  Leader**通知Follower节点确认提案**并写入自己的存储空间，随后所有的节点都拥有相同的数据

![image-20200926015244515](https://gitee.com/zero049/MyNoteImages/raw/master/image-20200926015244515.png)

若集群中**出现网络异常（分区）**，导致集群被分割，将出现多个Leader

![image-20200926015333820](https://gitee.com/zero049/MyNoteImages/raw/master/image-20200926015333820.png)

被分割出的非多数派集群将无法达到共识，即**脑裂**，如图中的A、B节点将无法确认提案（一般来说总节点都是奇数个，确保能保证存在一个多数派的情况）

![image-20200926015620842](https://gitee.com/zero049/MyNoteImages/raw/master/image-20200926015620842.png)

当集群再次连通时，将**只听从最新任期Leader**的指挥，旧Leader将退化为Follower，如图中B节点的Leader（任期1）需要听从D节点的Leader（任期2）的指挥，此时集群重新达到一致性状态

![image-20200926020121080](https://gitee.com/zero049/MyNoteImages/raw/master/image-20200926020121080.png)

最终连回来的节点会舍弃掉低任期 log未更新的数据（比如这里的3），从最新任期Leader的心跳包中拿到数据并更新

事实上宕机的节点重新连回来都需要保证一致性，从leader的log进行共识同步

![image-20200926020147426](https://gitee.com/zero049/MyNoteImages/raw/master/image-20200926020147426.png)

场景演示：https://raft.github.io/



### ZAB

说明：ZAB也是对Multi Paxos算法的改进，大部分和raft相同

和raft算法的主要区别：

1. 对于Leader的任期，raft叫做term，而ZAB叫做epoch
2. 在状态复制的过程中，raft的心跳从Leader向Follower发送，而ZAB则相反。

案例可见 [B站视频](https://www.bilibili.com/video/BV1TW411M7Fx?from=search&seid=5473721947148767319)

