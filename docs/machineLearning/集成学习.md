<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({ tex2jax: {inlineMath: [['$', '$']]}, messageStyle: "none" });
</script>

# 集成学习/模型融合（ModelEnsemble）
将一组弱/基预测器的预测结果进行融合/集成，
以实现一个强预测器，从而**可能**获得比单个预测器
更好的泛化能力。
模型融合的聚合策略：对所有弱预测器的预测结果
进行聚合所采用的策略。 常见的策略有：
1. **平均法：** 一般用于回归预测模型中。平均法包括一般
的平均和加权平均融合。
* 投票回归器(Voting Regressor)
* Boosting系列融合模型
2. **投票法(Voting)：** 一般用于分类模型。具体可分为绝
对多数投票（得票超过一半），相对多数投票（得票
最多），加权投票。
* bagging模型
* 投票分类器(Voting Classifier)
3. **学习法：** 通过另一个预测器(称为混合器或元学习器)
来实现聚合。常见的有Stacking和Blending两种。
* stacking一般使用交叉验证的方式
* Blending是建立一个Holdout集

## Bagging
bagging也叫做bootstrap aggregating，是在原始数据集选择S次后得到S个新数据集的一种技术。是一种**有放回**抽样。（元素可重复）
适用于弱分类器预测效果还行，但泛化能力不足。
例如：原始训练数据集{0,1,2,3,4,5,6,7,8,9}

Bootstrap采样
{7,2,6,7,5,4,8,8,1,0}——未采样3,9
{1,3,8,0,3,5,8,0,1,9}——未采样2,4,6,7
{2,9,4,2,7,9,3,0,1,0}——未采样3,5,6,8
<img src="https://gitee.com/zero049/MyNoteImages/raw/master/125426.png"  div align=center />
通过对划分出的不同训练集运用多种分类器训练模型，通过投票机制确定什么时候哪个模型会使数据拟合地更好
<img src="https://gitee.com/zero049/MyNoteImages/raw/master/Annotation 2019-10-10 203035.png"  div align=center />

## 随机森林（Ramdom Forest）

RF = 决策树 + Bagging + 随机属性选择

#### 步骤
1.样本的随机：从样本集中用bagging的方式，随机选择n个样本。
2.**特征**的随机：从所有属性d中随机选择k个属性（k＜d），然后从k个属性中选择最佳分割属性作为节点建立CART决策树。
3.重复以上两个步骤m次，建立m棵CART决策树。
4.这m棵CART决策树形成随机森林，通过投票表决结果，决定数据属于哪一类。
<img src="https://gitee.com/zero049/MyNoteImages/raw/master/125533.png"  div align=center />

## Boosting
Boosting中常用的是AdaBoost、XGBoost(最常用)、LightGBM.适用于弱分类器误差很大的时候。
AdaBoost是英文"Adaptive Boosting"（自适应增强）的缩写，它的自适应在于：前一个基本分类器被**错误分类**的样本的**权值会增大**，而**正确分类**的样本的**权值会减小**，并再次用来训练下一个基本分类器。同时，在**每一轮迭代中，加入一个新的弱分类器**，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数才确定最终的强分类器。
<img src="https://gitee.com/zero049/MyNoteImages/raw/master/Annotation 2019-10-10 194851.png"  div align=center />

如上图所示：
第一次迭代，对“样本4”预测错误，则加大样本中“样本4”出现的概率直到分类准确率提高到理想值
#### 步骤
Adaboost算法可以简述为三个步骤：
（1）首先，是初始化训练数据的权值分布$D_1$。假设有N个训练样本数据，则每一个训练样本最开始时，都被赋予相同的权值：$W_1=1/N$。
（2）然后，训练弱分类器$h_i$。具体训练过程中是：如果某个训练样本点，被弱分类器$h_i$准确地分类，那么在构造下一个训练集中，它对应的权值要减小；相反，如果某个训练样本点被错误分类，那么它的权值就应该增大。权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去。
（3）最后，将各个训练得到的弱分类器组合成一个强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。
&emsp;&emsp;换而言之，误差率低的弱分类器在最终分类器中占的权重较大，否则较小。

AdaBoost有两个版本
老版本训练过程：
<img src="https://gitee.com/zero049/MyNoteImages/raw/master/Annotation 2019-10-10 204703.png"  div align=center />
更新弱分类器权重过程：
<img src="https://gitee.com/zero049/MyNoteImages/raw/master/Annotation 2019-10-10 204130.png"  div align=center />
新版本训练过程

<img src="https://gitee.com/zero049/MyNoteImages/raw/master/Annotation 2019-10-10 213507.png"  div align=center />
对样本权重和弱分类器的更新与旧版本不同

### XGBoosting

### LightGBM


## Stacking
使用多个不同的分类器对训练集进预测，把预测得到的结果作为一个次级分类器的输入。次级分类器（在两层则为混合器）的输出是整个模型的预测结果。
对训练集在要进行划分，未用的训练集则要用于上一层的混合器的训练，也就是有多少层则需要对训练集划分出多少个子集。
<img src="https://gitee.com/zero049/MyNoteImages/raw/master/Annotation 2019-10-10 213356.png"  div align=center />
例如：
<img src="https://gitee.com/zero049/MyNoteImages/raw/master/Annotation 2019-10-11 100534.png"  div align=center />


## Voting
结合了多个不同种类的机器学习分类器，并且采用多数表决（majority vote）（硬投票）或者平均预测概率（软投票）的聚合方式来预测分类标签。
可以用于融合/集成一组同样表现良好的模型，以便平衡它们各自的弱点
#### 步骤
基于同一个训练集，可以训练出多个不同种类的分类器，包括逻辑回归模型，支持向量机， 随机森林等等。
聚合每个分类器的预测，然后将投票结果最多的结果作为预测的类别
<img src="https://gitee.com/zero049/MyNoteImages/raw/master/Annotation 2019-10-11 100925.png"  div align=center />

#### 软投票(Soft Voting)
<img src="https://gitee.com/zero049/MyNoteImages/raw/master/Annotation 2019-10-10 205004.png"  div align=center />