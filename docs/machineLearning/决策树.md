<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({ tex2jax: {inlineMath: [['$', '$']]}, messageStyle: "none" });
</script>

# 决策树
## 信息熵
一条信息的信息量大小和它的不确定性有直接的关系，要搞清楚一件非常非常不确定的事情，或者是我们一无所知的事情，需要了解大量信息->信息量的度量就等于不确定性的多少。
简言之，熵越大，事件越不确定，熵越小，事件越确定
### 信息熵公式
$$H[x] = -\sum_xp(x)\log_2{p(x)}$$
**例子:**

<img src="https://gitee.com/zero049/MyNoteImages/raw/master/160740.png" width = "1000" height = "120" div align=center />
<img src="https://gitee.com/zero049/MyNoteImages/raw/master/160959.png" width = "1900" height = "170" div align=center />

## ID3算法
**信息增益**
决策树会选择最大化信息增益来对结点进行划分。
信息增益计算：
$$Info(D)=-\sum_{i=1}^{m}p_i\log_2{(p_i)}\tag{1}$$
$$Info_A(D_j)=\sum_{j=1}^{v}\frac{|D_j|}{|D|}\times Info(D_j)\tag{2}$$
$$ Gain(A)=Info(D) - Info_A(D)\tag{3}$$

式子虽然看着复杂，通过例子即可理解
式子（1）是针对D事件（通常为分类结果）的信息熵的和
式子（2）是在A作为指标时，对不同的A以及A分类下不同的D，求得的信息熵之和
<img src="https://gitee.com/zero049/MyNoteImages/raw/master/103453.png"  div align=center />


## C4.5算法
**增益率**
信息增益的方法倾向于首先选择因子数较多(分支较多)的变量，但实际上这样的选择并不一定有效。
信息增益的改进：增益率
$$SplitInfo_A(D)=-\sum_{i=1}^{v} \frac{|D_j|}{|D|}\times\log_2{(\frac{|D_j|}{|D|})}\tag{4}$$
$$GrianRate(A)=\frac{Grain(A)}{SplitInfo_A}\tag{5}$$

也就是说增益率不再那么关注信息熵减少了多少，而是关注信息熵对应比率的下降

## CART算法
CART决策树的生成就是递归地构建二叉决策树的过程。
CART用基尼（Gini）系数最小化准则来进行特征选择，生成二叉树。
Gini系数计算：
$$Gini(D)=1-\sum_{i=1}^{m}p_i^2\tag{6}$$

$$Gini_A(D)=\frac{|D_1}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)\tag{7}$$
$$\Delta Gini(A)=Gini(D)-Gini_A(D)\tag{8} $$
思想是根据最大的$\Delta Gini(A)$选择作为子树的特征值，即选择信息增益最大的特征作为子树。

举例理解：
如图所示的特征判断是否拖欠贷款
<img src="https://gitee.com/zero049/MyNoteImages/raw/master/183254.png"  div align=center />
根节点的Gini系数：
$$Gini(是否拖欠贷款)=1-(\frac{3}{10})^2-(\frac{7}{10})^2 = 0.42$$
分别计算它们的Gini系数增益，取Gini系数增益值最大的属性作为决策树的根节点属性。
<img src="https://gitee.com/zero049/MyNoteImages/raw/master/163954.png"  div align=center />
显然当分类都为yes或都为no时Gini系数为零
<img src="https://gitee.com/zero049/MyNoteImages/raw/master/104057.png"  div align=center />
<img src="https://gitee.com/zero049/MyNoteImages/raw/master/103911.png"  div align=center />
<img src="https://gitee.com/zero049/MyNoteImages/raw/master/104739.png"  div align=center />
<img src="https://gitee.com/zero049/MyNoteImages/raw/master/104651.png"  div align=center />

## 剪枝
防止过拟合
**预剪枝**
在建立决策树之前预先把无用的特征剔除
**后剪枝**
建立决策树之后，删减不必要的分支，如某一个分支极少的情况
<img src="https://gitee.com/zero049/MyNoteImages/raw/master/110700.png"  div align=center />
如图假设$A_3$有99个类B、2个类A就没有必要再分枝

### 决策树优缺点
**优点：**
* 小规模数据集有效

**缺点：**
* 处理连续变量不好
* 类别较多时，错误增加的比较快
* 不能处理大量数据


